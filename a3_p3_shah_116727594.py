# -*- coding: utf-8 -*-
"""a3_p3_Shah_116727594.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yR5o4o36eHn7AsisnNecGc_gg2pEZ3HF
"""

# CHECKPOINT 3.1

from datasets import load_dataset
from sentence_transformers import SentenceTransformer

def get_unique_ctx_examples(squad, n=500):
    context2idx = {}

    for i, entry in enumerate(squad['validation']):
        if not entry['context'] in context2idx:
            context2idx[entry['context']] = []
        context2idx[entry['context']].append(i)

    queries, contexts, answers = [], [], []

    for k, v in context2idx.items():
        idx = v[0]
        queries.append(squad['validation'][idx]['question'])
        contexts.append(squad['validation'][idx]['context'])
        answers.append(squad['validation'][idx]['answers'])

        if len(queries) == n:
            break

    return queries, contexts, answers


squad = load_dataset("squad")
model = SentenceTransformer("all-MiniLM-L6-v2")
queries, contexts, answers = get_unique_ctx_examples(squad)

import numpy as np
from sentence_transformers import SentenceTransformer, util


def retrieve(contexts, embeddings, query):
    query_embedding = model.encode(query, convert_to_tensor=True)

    # Compute cosine similarities between the query and all context embeddings
    cosine_scores = util.cos_sim(query_embedding, embeddings)[0]

    idx = int(np.argmax(cosine_scores))

    ret_context = contexts[idx]

    return idx, ret_context

embeddings = model.encode(contexts, convert_to_tensor=True)



correct = 0

for i, query in enumerate(queries):
    idx, ret_context = retrieve(contexts, embeddings, query)

    # Ground truth context for the query
    gold_context = contexts[i]

    if ret_context == gold_context:
        correct += 1

accuracy = correct / len(queries)
print(f"Retrieval Accuracy: {correct} / {len(queries)} = {accuracy:.2f}")

# CHECKPOINT 3.2

# Sentence embedding model
embedder = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = embedder.encode(contexts, convert_to_tensor=True)

# Phi-3 for generation
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")
phi3_model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-3-mini-4k-instruct", torch_dtype=torch.bfloat16, device_map="auto"
)

def generate_response(model, tokenizer, query, ret_context):
    messages = [
        {
            "role": "system",
            "content": "You are a helpful AI assistant. "
                       "Provide one Answer ONLY to the following query based on the context provided below. "
                       "Do not generate or answer any other questions. "
                       "Do not make up or infer any information that is not directly stated in the context. "
                       "Provide a concise answer."
                       f"\nContext: {ret_context}"
        },
        {
            "role": "user",
            "content": query
        }
    ]

    prompt = ""
    for message in messages:
        role = message["role"]
        content = message["content"]
        if role == "system":
            prompt += f"<|system|>\n{content}\n"
        elif role == "user":
            prompt += f"<|user|>\n{content}\n<|assistant|>\n"

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=150,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Clean up: Extract only the assistant's reply after "<|assistant|>"
    if "<|assistant|>" in response:
        response = response.split("<|assistant|>")[-1].strip()
    elif prompt in response:
        response = response.replace(prompt, "").strip()

    return response

import random
from sentence_transformers import util

def run_generation(embedder, phi3_model, tokenizer, queries, contexts, answers, embeddings):
    correct_samples = []
    incorrect_samples = []

    for i, query in enumerate(queries):
        query_embedding = embedder.encode(query, convert_to_tensor=True)
        cosine_scores = util.cos_sim(query_embedding, embeddings)[0]
        idx = int(torch.argmax(cosine_scores))
        ret_context = contexts[idx]
        gold_context = contexts[i]

        sample = {
            "query": query,
            "ret_context": ret_context,
            "gold_context": gold_context,
            "gold_answer": answers[i]['text'][0],
            "idx": i
        }

        if ret_context.strip() == gold_context.strip():
            correct_samples.append(sample)
        else:
            incorrect_samples.append(sample)

        if len(correct_samples) >= 5 and len(incorrect_samples) >= 5:
            break

    if len(correct_samples) < 5 or len(incorrect_samples) < 5:
        print("Not enough correct/incorrect samples found. Try relaxing context match or increasing 'n'.")
        return

    selected_samples = random.sample(correct_samples, 5) + random.sample(incorrect_samples, 5)

    for i, sample in enumerate(selected_samples):
          response = generate_response(phi3_model, tokenizer, sample['query'], sample['ret_context'])

          print(f"\n--- Example {i+1} ---")
          print(f"Question: {sample['query']}")
          print(f"Actual Answer: {sample['gold_answer']}")
          print(f"Generated Answer: {response}")

run_generation(embedder, phi3_model, tokenizer, queries, contexts, answers, embeddings)

