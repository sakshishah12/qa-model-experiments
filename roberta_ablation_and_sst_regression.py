# -*- coding: utf-8 -*-
"""a3_p3_Shah_116727594.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fcl4KHpdk5L8b05ev3Drv98jqiJbKUDw
"""

# CHECKPOINT 3.1


import torch.nn as nn
import torch

def reinitialize_weights(model):
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if hasattr(module, "padding_idx") and module.padding_idx is not None:
                with torch.no_grad():
                    module.weight[module.padding_idx].fill_(0)
        elif isinstance(module, nn.LayerNorm):
            nn.init.ones_(module.weight)
            nn.init.zeros_(module.bias)

import torch
import torch.nn as nn

def modify_attention_to_shared_kqv(model):
    for layer in model.roberta.encoder.layer:
        attention = layer.attention.self

        # Get original Q and K weights/biases
        q_w = attention.query.weight.data
        k_w = attention.key.weight.data
        q_b = attention.query.bias.data
        k_b = attention.key.bias.data

        # Compute mean
        shared_w = (q_w + k_w) / 2
        shared_b = (q_b + k_b) / 2

        # Create new shared linear layer
        shared_layer = nn.Linear(q_w.size(1), q_w.size(0))
        shared_layer.weight = nn.Parameter(shared_w.clone())
        shared_layer.bias = nn.Parameter(shared_b.clone())

        # Assign the shared layer to all three (Q, K, V)
        attention.query = shared_layer
        attention.key = shared_layer
        attention.value = shared_layer

#2.1.3
from transformers import AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert/distilroberta-base", num_labels=2
)
from transformers import RobertaForSequenceClassification

# Custom RobertaOutput with no residual
class RobertaOutputNoRes(nn.Module):
    def __init__(self, original_output):
        super().__init__()
        self.dense = original_output.dense
        self.dropout = original_output.dropout
        self.LayerNorm = original_output.LayerNorm

    def forward(self, hidden_states, input_tensor=None):  # input_tensor unused
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.LayerNorm(hidden_states)  # No residual added
        return hidden_states

# Custom RobertaLayer with no residuals
class RobertaLayerNoRes(nn.Module):
    def __init__(self, original_layer):
        super().__init__()
        self.attention = original_layer.attention
        self.intermediate = original_layer.intermediate
        self.output = RobertaOutputNoRes(original_layer.output)  # ðŸ‘ˆ swapped
        self.LayerNorm1 = original_layer.attention.output.LayerNorm
        self.LayerNorm2 = original_layer.output.LayerNorm

    def forward(self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_value=None, output_attentions=False):
        attention_outputs = self.attention(
            hidden_states,
            attention_mask=attention_mask,
            head_mask=head_mask,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=encoder_attention_mask,
            past_key_value=past_key_value,
            output_attentions=output_attentions,
        )
        attention_output = attention_outputs[0]
        attention_output = self.LayerNorm1(attention_output)  # No residual

        intermediate_output = self.intermediate(attention_output)
        layer_output = self.output(intermediate_output)  # Only one argument
        layer_output = self.LayerNorm2(layer_output)  # No residual

        outputs = (layer_output,) + attention_outputs[1:]
        return outputs


# Swap encoder layers with no-residual variants
for i in range(len(model.roberta.encoder.layer)):
    model.roberta.encoder.layer[i] = RobertaLayerNoRes(model.roberta.encoder.layer[i])

# Final no-residual model
distilrb_nores = model

from transformers import AutoModelForSequenceClassification

# 1. distilrbrand â€” randomly initialized
model1 = AutoModelForSequenceClassification.from_pretrained(
    "distilbert/distilroberta-base", num_labels=2
)
reinitialize_weights(model1)
distilrbrand = model1

# 2. distilRB-KQV â€” shared Q=K=V attention weights
model2 = AutoModelForSequenceClassification.from_pretrained(
    "distilbert/distilroberta-base", num_labels=2
)
modify_attention_to_shared_kqv(model2)
distilRBKQV = model2



from datasets import load_dataset
dataset_train=load_dataset("google/boolq",split="train")
dataset_val = load_dataset("google/boolq",split="validation")

from transformers import AutoTokenizer

# Tokenizer (shared across models)
tokenizer = AutoTokenizer.from_pretrained("distilbert/distilroberta-base")

def preprocess(example):
    # Tokenize inputs
    tokenized = tokenizer(
        example["question"],
        example["passage"],
        truncation=True,
        padding="max_length",
        max_length=512
    )

    # Convert 'answer' (True/False) to int (1/0)
    tokenized["labels"] = int(example["answer"])
    return tokenized

tokenized_train = dataset_train.map(preprocess, batched=False)
tokenized_val = dataset_val.map(preprocess, batched=False)

tokenized_train.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
tokenized_val.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

from transformers import Trainer, TrainingArguments
import torch

def get_training_args(run_name):
    return TrainingArguments(
        output_dir=f"./results/{run_name}",
        run_name=run_name,
        eval_strategy="epoch",
        save_strategy="no",
        learning_rate=1e-5,
        per_device_train_batch_size=8,
        num_train_epochs=1,
        weight_decay=0.01,
        fp16=True,
        logging_dir=f"./logs/{run_name}",
        logging_strategy="epoch",
        report_to="none",
    )

def fine_tune_and_eval(model, name):
    args = get_training_args(run_name=name)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_val,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )

    print(f"\nTraining {name}...")
    trainer.train()
    print(f"Evaluating {name}...")
    metrics = trainer.evaluate()
    acc = metrics["eval_accuracy"]
    f1 = metrics["eval_f1"]
    print(f"{name}: overall acc: {acc:.3f}, f1: {f1:.3f}")
    return acc, f1

# Train & eval on distilRB variants
results = {}

# Skip distilRB-rand as instructed
# results["distilroberta"] = fine_tune_and_eval(distilroberta, "distilroberta")
results["distilRB-KQV"] = fine_tune_and_eval(distilRBKQV, "distilRB-KQV")
results["distilRB-nores"] = fine_tune_and_eval(distilrb_nores, "distilRB-nores")



from sklearn.metrics import accuracy_score, f1_score
import numpy as np
from transformers import EvalPrediction

def compute_metrics(p: EvalPrediction):
    preds = np.argmax(p.predictions, axis=1)
    labels = p.label_ids
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1": f1_score(labels, preds)
    }

from transformers import Trainer, TrainingArguments
import torch

# Define dummy training args for evaluation
eval_args = TrainingArguments(
    output_dir="./results/distilRB-rand-eval",
    per_device_eval_batch_size=8,
    report_to="none",
    fp16=True,
)

# Move model to device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
distilrbrand.to(device)

# Initialize Trainer
rand_trainer = Trainer(
    model=distilrbrand,
    args=eval_args,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# Evaluate
print("Evaluating distilRB-rand (no fine-tuning)...")
rand_metrics = rand_trainer.evaluate()
print(f"distilRB-rand: overall acc: {rand_metrics['eval_accuracy']:.3f}, f1: {rand_metrics['eval_f1']:.3f}")

for name in ["distilRB-KQV", "distilRB-nores"]:
    acc, f1 = results[name]
    print(f"{name}: overall acc: {acc:.3f}, f1: {f1:.3f}")
print(f"distilRB-rand: overall acc: {rand_metrics['eval_accuracy']:.3f}, f1: {rand_metrics['eval_f1']:.3f}")

from datasets import load_dataset

sst = load_dataset('stanfordnlp/sst', trust_remote_code=True)
print(sst['train'].column_names)

import torch
from datasets import load_dataset
from transformers import AutoTokenizer, Trainer, TrainingArguments
from torch.optim import AdamW
from sklearn.metrics import mean_absolute_error, r2_score
from scipy.stats import pearsonr
import numpy as np

# Load SST with trust enabled
sst = load_dataset('stanfordnlp/sst', trust_remote_code=True)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("distilbert/distilroberta-base")

# Tokenization function
def tokenize_fn(example):
    return tokenizer(example['sentence'], padding='max_length', truncation=True, max_length=128)

# Tokenize
tokenized_train = sst['train'].map(tokenize_fn, batched=True)
tokenized_val = sst['validation'].map(tokenize_fn, batched=True)
tokenized_test = sst['test'].map(tokenize_fn, batched=True)

# Add regression labels
tokenized_train = tokenized_train.rename_column("label", "labels")
tokenized_val = tokenized_val.rename_column("label", "labels")
tokenized_test = tokenized_test.rename_column("label", "labels")

tokenized_train.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
tokenized_val.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
tokenized_test.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

# Define model
from transformers import RobertaModel, RobertaPreTrainedModel
import torch.nn as nn

class DistilRobertaForRegression(RobertaPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.roberta = RobertaModel(config)
        self.regressor = nn.Linear(config.hidden_size, 1)
        self.init_weights()

    def forward(self, input_ids=None, attention_mask=None, labels=None):
        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)
        logits = self.regressor(outputs.last_hidden_state[:, 0, :]).squeeze(-1)

        loss = None
        if labels is not None:
            loss_fct = nn.MSELoss()
            loss = loss_fct(logits, labels.float())

        return {"loss": loss, "logits": logits} if loss is not None else {"logits": logits}

# Instantiate model
from transformers import RobertaConfig

config = RobertaConfig.from_pretrained("distilbert/distilroberta-base")
regression_model = DistilRobertaForRegression(config)
#regression_model.roberta.load_state_dict(torch.load("PATH/TO/PRETRAINED/distilroberta/pytorch_model.bin", map_location="cpu"), strict=False)  # optional
regression_model.to("cuda")

training_args = TrainingArguments(
    output_dir="./regression_output",
    eval_strategy="epoch",
    fp16=True,
    save_strategy="epoch",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=5,
    learning_rate=1e-5,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    report_to="none",  
    run_name="distilroberta-regression"
)

# Evaluation metrics
def compute_metrics(pred):
    preds = pred.predictions
    labels = pred.label_ids
    mae = mean_absolute_error(labels, preds)
    r, _ = pearsonr(labels, preds)
    return {"mae": mae, "pearson_r": r}

trainer = Trainer(
    model=regression_model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    compute_metrics=compute_metrics,
    optimizers=(AdamW(regression_model.parameters(), lr=1e-5), None)
)


trainer.train()

val_metrics = trainer.evaluate(eval_dataset=tokenized_val)
test_metrics = trainer.evaluate(eval_dataset=tokenized_test)

print(f"\nValidation: mae: {val_metrics['eval_mae']:.3f}, r: {val_metrics['eval_pearson_r']:.3f}")
print(f"     Test: mae: {test_metrics['eval_mae']:.3f}, r: {test_metrics['eval_pearson_r']:.3f}")

